version: "1"

service:
  name: ollama-ai-assistant
  description: "Ollama llama3.2:1b AI Assistant â€” React frontend + FastAPI backend"

build:
  context: .
  dockerfile: Dockerfile

deploy:
  port: 8080
  minInstances: 1
  maxInstances: 2
  resources:
    cpu: "2"
    memory: "4Gi"

environment:
  - name: PORT
    value: "8080"
  - name: MODEL
    value: "llama3.2:1b"
  - name: OLLAMA_HOST
    value: "0.0.0.0:11434"
  - name: PYTHONUNBUFFERED
    value: "1"

health:
  path: /health
  initialDelaySeconds: 60      # extra time for ollama serve + model pull on cold start
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 5
