version: "1"

services:
  app:
    name: ollama-ai-assistant
    dockerfile: Dockerfile        # root-level single Dockerfile
    port: 8080
    resources:
      memory: 4096               # MB â€” Ollama + llama3.2:1b requires ~3-4 GB
      cpu: 2
    env:
      PORT: "8080"
      MODEL: "llama3.2:1b"
      OLLAMA_HOST: "0.0.0.0:11434"
    health_check:
      path: /health
      interval: 30
      timeout: 15
      retries: 5                 # extra retries for model pull on cold start
