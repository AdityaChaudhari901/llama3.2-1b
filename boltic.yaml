# Application name
app: ollama-ai-assistant

# Deployment region
region: asia-south1

# Build settings — use our Dockerfile (skips Python auto-detection)
build:
  dockerfile: Dockerfile

# Exposed port
http:
  port: 8080

# Environment variables
env:
  PORT: "8080"
  MODEL: "llama3.2:1b"
  OLLAMA_HOST: "0.0.0.0:11434"

# Resource configuration
resources:
  memory: "4096"       # MB — Ollama + llama3.2:1b needs ~3-4 GB
  max_memory: "4096"
  cpus: 2
  cpu_kind: performance
